# -*- coding: utf-8 -*-
"""DATA301 Project Timing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1an_FKWJzuvUbMelW7gWK1HxAuG3-FZf8

##Download data
Using a filtered dataset collected from Food.com (https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions)

##Set up a local spark cluster
"""

# Commented out IPython magic to ensure Python compatibility.
# %env PYTHONHASHSEED 3
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip install -q pyspark

from math import sqrt
import pyspark
from pyspark import SparkConf, SparkContext
from pyspark.sql import *
spark = SparkSession.builder.master("local[*]").appName('SparkExample').config(
    "spark.executor.memory", "1g").config("spark.ui.port", "4050"
        ).getOrCreate()
sc = spark.sparkContext

"""Loading in the data from Google Drive"""

import urllib.request

filename = 'recipes.csv'
urllib.request.urlretrieve('https://drive.google.com/uc?export=download&confirm=t&id=1jF-nOmLeR-xxv7XiVZ2xlawPqNoZ3auE', filename)

filename = 'interactions.csv'
urllib.request.urlretrieve('https://drive.google.com/uc?export=download&confirm=t&id=14VAyOZfzZWv11ac3rItNT1EtC0IEpDkM', filename)

"""Loading each file of data into an rdd"""

import csv

#recipes
#read the file as an RDD of lines
recipe_lines = sc.textFile('recipes.csv')
#parse the data into rows, using the csv reader
recipe_rows = recipe_lines.map(lambda line: list(csv.reader([line]))[0])

#interactions
#read the file as an RDD of lines
interaction_lines = sc.textFile('interactions.csv')
#parse the data into rows, using the csv reader
interaction_rows = interaction_lines.map(lambda line: list(csv.reader([line]))[0])

print(recipe_rows.count())
three = recipe_rows.zipWithIndex().filter(lambda x: x[1] < 200837).map(lambda x: x[0])
two = recipe_rows.zipWithIndex().filter(lambda x: x[1] < 133891).map(lambda x: x[0])
one = recipe_rows.zipWithIndex().filter(lambda x: x[1] < 66945).map(lambda x: x[0])
print(one.count())

#used to get the ingredinets to be a list
import ast

import time

time_start = time.time()


#filtering the data to remove any rows which may be empty or not follow the structure.
# specifically checking the values of the attributes at indexes 0, 1, and 10 because they are the attributes we use in the code.
# the last check is removing the heading line
filtered_rows = one.filter(lambda row: len(row) == 12 and row[0] is not None and row[1] is not None and row[10] is not None and row[10] != 'ingredients')

#converting the value for ingredients into an actual list, checking that the input is a list before converting and including it,
# otherwise putting it as None and then filtering out the None cases
recipes_rdd = filtered_rows.map(lambda x: (x[0], x[1], ast.literal_eval(x[10]) if x[10].startswith('[') and x[10].endswith(']') else None)).filter(lambda x: x[2] is not None)





#just getting the ingredients in order to get a set of the ingredients
ingredients_listed = recipes_rdd.map(lambda x: (x[2]))
filtered_ingredients = ingredients_listed.filter(lambda x: isinstance(x, list))

#flat mapping to get them all together, and trying to remove any remaining artifacts of the initial csv layout
ingredients_all = recipes_rdd.flatMap(lambda x: [str(i).replace("[", "").replace("]", "") for i in x[2] if "[" not in str(i) and "]" not in str(i)])

#making a list of unique ingredients
unique_ingredients_list = list(set(ingredients_all.collect()))



#creating a function which converts a list of ingredients to a sparse matrix representation -Using pyspark's SparseVector
from pyspark.mllib.linalg import SparseVector

def convert_to_sparse_vector(ingredients):
    indices = []
    values = []

    for ingredient in ingredients:
        if ingredient in unique_ingredients_list:
            index = unique_ingredients_list.index(ingredient)
            indices.append(index)
            values.append(1)

    num_unique_ingredients = len(unique_ingredients_list)

    # Sort indices and values based on indices
    sorted_indices_values = sorted(zip(indices, values))
    sorted_indices, sorted_values = zip(*sorted_indices_values)

    sparse_vector = SparseVector(num_unique_ingredients, sorted_indices, values)

    return sparse_vector



#using the function on the recipe_rdd to convert all the ingredient lists to sparse vectors
sparse_matrix_rdd = recipes_rdd.map(lambda x: (x[0], x[1], convert_to_sparse_vector(x[2])))



test_recipe_ingredients = sparse_matrix_rdd.map(lambda x: x[2]).take(1)[0]




#code from lecturer
def cosine_similarity_sv(x, y):
  """
  Computes cosine similarity on spark ML SparseVectors x and y
  """
  #multiplication of two 1's is a 1, anything else is a 0, so bitset dot product
  # is equivalent to bitwise and followed by a count of all nonzero elements
  dot_product = x.dot(y)
  #magnitude is sqrt(sum of squares), square of 0 is 0 and square of 1 is 1,
  #so sum of squares for us is equivalent to count of all nonzero elements
  mag_x = sqrt(x.numNonzeros())
  mag_y = sqrt(y.numNonzeros())
  return dot_product / (mag_x*mag_y)




similarities = sparse_matrix_rdd.map(lambda x: (x[0], x[1], cosine_similarity_sv(test_recipe_ingredients, x[2])))

top_15 = similarities.takeOrdered(15, key=lambda x: -x[2])

#excluding the recipes with a similarity of 0
similarities_no_zero = similarities.filter(lambda x: x[2] != 0.0)
bottom_15 = similarities_no_zero.takeOrdered(15, key=lambda x: x[2])





def isfloat(num):
    try:
        float(num)
        return True
    except ValueError:
        return False

#getting all the recipe_ids from the returned recipes
low_and_high_recipes = top_15[1] + bottom_15[1]

#filter so the recipes being returned have their average rating returned
recipe_id_to_rating_rdd_all = interaction_rows.filter(lambda x: x[1] in low_and_high_recipes)

#remove rows missing the unnecessary attributes and the first heading row
recipe_id_to_rating_rdd_all = interaction_rows.filter(lambda x: len(x)==5 and x[1]!=None and x[3]!=None and isfloat(x[3]) and x[0]!='user_id')

#remove unnecessary attributes
recipe_id_to_rating_rdd = recipe_id_to_rating_rdd_all.map(lambda x: (x[1], float(x[3])))

#get the sum of ratings for each recipe
sum_ratings_rdd = recipe_id_to_rating_rdd.reduceByKey(lambda a, b: a + b)

#get the count of ratings for each recipe
count_ratings_rdd = recipe_id_to_rating_rdd.mapValues(lambda x: 1).reduceByKey(lambda a, b: a + b)

#get the average rating for each recipe
average_ratings_rdd = sum_ratings_rdd.join(count_ratings_rdd).mapValues(lambda x: x[0] / x[1])

#collecting them because there will only be 30 key value pairs.
average_ratings = average_ratings_rdd.collect()





ratings = dict(average_ratings)

top_15_sorted_by_rating = sorted(top_15, key=lambda x: ratings.get(x[1], 0), reverse=True)

bottom_15_sorted_by_rating = sorted(bottom_15, key=lambda x: ratings.get(x[1], 0), reverse=True)

print("Recipes with the most amount of ingredients in common, ordered by average rating:")
for x in top_15_sorted_by_rating:
  print(x)

print("\nRecipes with the least amount of ingredients in common, ordered by average rating:")
for x in bottom_15_sorted_by_rating:
  print(x)






time_end = time.time()
elapsed_time = time_end - time_start
print("elapsed time is %s" % str(elapsed_time))



# Single Core Timings -whole dataset, then divided it by 4
# 903.0195543766022 - 4 (whole dataset)
# 640.648243188858 - 3
# 399.7633333206177 - 2
# 195.56268429756165 - 1

#Google Cloud Timings
#1115.723994255066 - whole dataset (1 core) -IGNORE THIS ONE because it doesn't increase the number of processes with the size, its going the other way around

#502.10525822639465 - one and 1 P             238.76158809661865 -same thing run again
#471.3831477165222 - two and 4 P              459.93190932273865 -same thing run again (might be more accurate)
#762.7371921539307 - three and 8 P
#1020.0644452571869 - four and 16 P

#Graph
import matplotlib.pyplot as plt
import numpy as np

timings = np.array([1020.0644452571869, 762.7371921539307, 471.3831477165222, 238.76158809661865])
x = np.array([267783, 200837, 133891, 66945])
P = np.array([16, 8, 4, 1])
y = np.array(timings)

#create basic scatterplot
plt.plot(x, y, 'o', label="Raw Data")

plt.xlabel("Size of Data Set")
plt.ylabel("Time [s]")

#obtain m (slope) and b(intercept) of linear regression line
m, b = np.polyfit(x, y, 1)

#add linear regression line to scatterplot
plt.plot(x, m*x+b, label="Fitted Data")

#second x-axis for P
plt.twiny()
#second x-axis ticks
plt.xticks(x, P)
#label for the second x-axis
plt.xlabel("P")

plt.legend()

import matplotlib.pyplot as plt
import numpy as np
x = np.array([267783, 200837, 133891, 66945])
P = np.array([16, 8, 4, 1])
y = np.array([1020.0644452571869, 762.7371921539307, 471.3831477165222, 238.76158809661865])

fig, ax1 = plt.subplots()  # Create the figure and first set of axes

# Plotting the data on the first x-axis
ax1.scatter(x, y, label="Raw Data")

#obtain m (slope) and b(intercept) of linear regression line
m, b = np.polyfit(x, y, 1)

#add linear regression line to scatterplot
ax1.plot(x, m*x+b, label="Linear Regression", color="#ff7f0e")

# set x ticks
ax1.set_xticks(x)

# Set labels and color for the first x-axis
ax1.set_xlabel('Size of Data Set')
ax1.set_ylabel('Time [s]')

ax1.legend()

ax1.grid()

#second x-axis for P
ax2 = ax1.twiny()  # Create a second axes that shares the same y-axis

#second x-axis ticks
ax2.set_xticks([0.85, 6.6, 12.4, 18.15]) # set the ticks so that the whole thing lines up
ax2.set_xticklabels([1, 4, 8, 16])
ax2.set_xlim(None, 19)

ax2.set_xlabel("Number of Processors")

"""Defining the test recipe to be the sparse matrix of the first recipe in the rdd"""



"""Defining a function to compute the cosine similarity between two lists of integers

Similarity is 1 when two vectors are identical (where the distance would be 0)
"""



"""Computing the cosine similarity between the test_recipe and each of the recipes in the dataset."""



"""Computing the average ratings for each recipe"""



"""ordering by the average rating"""

