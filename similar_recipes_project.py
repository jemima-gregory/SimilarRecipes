# -*- coding: utf-8 -*-
"""DATA301 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jrMXPPVbwla-FMv1XgvzBjYATcO0Q7tE

##Download data
Using a filtered dataset collected from Food.com (https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions)

##Set up a local spark cluster
"""

# Commented out IPython magic to ensure Python compatibility.
# %env PYTHONHASHSEED 3
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip install -q pyspark

from math import sqrt
import pyspark
from pyspark import SparkConf, SparkContext
from pyspark.sql import *
spark = SparkSession.builder.master("local[*]").appName('SparkExample').config(
    "spark.executor.memory", "1g").config("spark.ui.port", "4050"
        ).getOrCreate()
sc = spark.sparkContext

"""Loading in the data from Google Drive"""

import urllib.request

filename = 'recipes.csv'
urllib.request.urlretrieve('https://drive.google.com/uc?export=download&confirm=t&id=1jF-nOmLeR-xxv7XiVZ2xlawPqNoZ3auE', filename)

filename = 'interactions.csv'
urllib.request.urlretrieve('https://drive.google.com/uc?export=download&confirm=t&id=14VAyOZfzZWv11ac3rItNT1EtC0IEpDkM', filename)

"""Loading each file of data into an rdd"""

import csv

#recipes
#read the file as an RDD of lines
recipe_lines = sc.textFile('recipes.csv')
#parse the data into rows, using the csv reader
recipe_rows = recipe_lines.map(lambda line: list(csv.reader([line]))[0])

#interactions
#read the file as an RDD of lines
interaction_lines = sc.textFile('interactions.csv')
#parse the data into rows, using the csv reader
interaction_rows = interaction_lines.map(lambda line: list(csv.reader([line]))[0])

#used to get the ingredinets to be a list
import ast

#filtering the data to remove any rows which may be empty or not follow the structure.
# specifically checking the values of the attributes at indexes 0, 1, and 10 because they are the attributes we use in the code.
# the last check is removing the heading line
filtered_rows = recipe_rows.filter(lambda row: len(row) == 12 and row[0] is not None and row[1] is not None and row[10] is not None and row[10] != 'ingredients')

#converting the value for ingredients into an actual list, checking that the input is a list before converting and including it,
# otherwise putting it as None and then filtering out the None cases
recipes_rdd = filtered_rows.map(lambda x: (x[0], x[1], ast.literal_eval(x[10]) if x[10].startswith('[') and x[10].endswith(']') else None)).filter(lambda x: x[2] is not None)

#to see what the filtering leaves us.
for row in recipes_rdd.take(10):
  print(row)

#just getting the ingredients in order to get a set of the ingredients
ingredients_listed = recipes_rdd.map(lambda x: (x[2]))
filtered_ingredients = ingredients_listed.filter(lambda x: isinstance(x, list))

#flat mapping to get them all together, and trying to remove any remaining artifacts of the initial csv layout
ingredients_all = recipes_rdd.flatMap(lambda x: [str(i).replace("[", "").replace("]", "") for i in x[2] if "[" not in str(i) and "]" not in str(i)])

#making a list of unique ingredients
unique_ingredients_list = list(set(ingredients_all.collect()))

#creating a function which converts a list of ingredients to a sparse matrix representation -Using pyspark's SparseVector
from pyspark.mllib.linalg import SparseVector

def convert_to_sparse_vector(ingredients):
    indices = []
    values = []

    for ingredient in ingredients:
        if ingredient in unique_ingredients_list:
            index = unique_ingredients_list.index(ingredient)
            indices.append(index)
            values.append(1)

    num_unique_ingredients = len(unique_ingredients_list)

    # Sort indices and values based on indices
    sorted_indices_values = sorted(zip(indices, values))
    sorted_indices, sorted_values = zip(*sorted_indices_values)

    sparse_vector = SparseVector(num_unique_ingredients, sorted_indices, values)

    return sparse_vector

#using the function on the recipe_rdd to convert all the ingredient lists to sparse vectors
sparse_matrix_rdd = recipes_rdd.map(lambda x: (x[0], x[1], convert_to_sparse_vector(x[2])))
print(sparse_matrix_rdd.take(1))

#testing retrieval of ingredient names
#rdd_items = sparse_matrix_rdd.map(lambda x: (x[0], x[1], ([unique_ingredients_list[i] for i in x[2]])))
#print(rdd_items.take(1))

"""Defining the test recipe to be the sparse matrix of the first recipe in the rdd"""

test_recipe_ingredients = sparse_matrix_rdd.map(lambda x: x[2]).take(1)[0]
print(test_recipe_ingredients)

"""Defining a function to compute the cosine similarity between two lists of integers

Similarity is 1 when two vectors are identical (where the distance would be 0)
"""

#code from lecturer
def cosine_similarity_sv(x, y):
  """
  Computes cosine similarity on spark ML SparseVectors x and y
  """
  #multiplication of two 1's is a 1, anything else is a 0, so bitset dot product
  # is equivalent to bitwise and followed by a count of all nonzero elements
  dot_product = x.dot(y)
  #magnitude is sqrt(sum of squares), square of 0 is 0 and square of 1 is 1,
  #so sum of squares for us is equivalent to count of all nonzero elements
  mag_x = sqrt(x.numNonzeros())
  mag_y = sqrt(y.numNonzeros())
  return dot_product / (mag_x*mag_y)

"""Computing the cosine similarity between the test_recipe and each of the recipes in the dataset."""

similarities = sparse_matrix_rdd.map(lambda x: (x[0], x[1], cosine_similarity_sv(test_recipe_ingredients, x[2])))
print(similarities.take(5))

top_15 = similarities.takeOrdered(15, key=lambda x: -x[2])
print(top_15)

#excluding the recipes with a similarity of 0
similarities_no_zero = similarities.filter(lambda x: x[2] != 0.0)
bottom_15 = similarities_no_zero.takeOrdered(15, key=lambda x: x[2])
print(bottom_15)

"""Computing the average ratings for each recipe"""

def isfloat(num):
    try:
        float(num)
        return True
    except ValueError:
        return False

#getting all the recipe_ids from the returned recipes
low_and_high_recipes = top_15[1] + bottom_15[1]

#filter so the recipes being returned have their average rating returned
recipe_id_to_rating_rdd_all = interaction_rows.filter(lambda x: x[1] in low_and_high_recipes)

#remove rows missing the unnecessary attributes and the first heading row
recipe_id_to_rating_rdd_all = interaction_rows.filter(lambda x: len(x)==5 and x[1]!=None and x[3]!=None and isfloat(x[3]) and x[0]!='user_id')

#remove unnecessary attributes
recipe_id_to_rating_rdd = recipe_id_to_rating_rdd_all.map(lambda x: (x[1], float(x[3])))

#get the sum of ratings for each recipe
sum_ratings_rdd = recipe_id_to_rating_rdd.reduceByKey(lambda a, b: a + b)

#get the count of ratings for each recipe
count_ratings_rdd = recipe_id_to_rating_rdd.mapValues(lambda x: 1).reduceByKey(lambda a, b: a + b)

#get the average rating for each recipe
average_ratings_rdd = sum_ratings_rdd.join(count_ratings_rdd).mapValues(lambda x: x[0] / x[1])

#collecting them because there will only be 30 key value pairs.
average_ratings = average_ratings_rdd.collect()

"""ordering by the average rating"""

ratings = dict(average_ratings)

top_15_sorted_by_rating = sorted(top_15, key=lambda x: ratings.get(x[1], 0), reverse=True)

bottom_15_sorted_by_rating = sorted(bottom_15, key=lambda x: ratings.get(x[1], 0), reverse=True)

print("Recipes with the most amount of ingredients in common, ordered by average rating:")
for x in top_15_sorted_by_rating:
  print(x)

print("\nRecipes with the least amount of ingredients in common, ordered by average rating:")
for x in bottom_15_sorted_by_rating:
  print(x)